《机器学习基石》课程笔记  
整个基石课程分成四个部分：\
1.When Can Machine Learn?\
2.Why Can Machine Learn?\
3.How Can Machine Learn?\
4.How Can Machine Learn Better?\
本篇学习笔记主要记录自己在学习Lecture 1: The Learning Problem和Lecture 2: Learning to Answer Yes/No这两节课的内容进行总结。\
1.What is machine learning?\
&ensp;&ensp;&ensp;&ensp;人类学习是通过大量的观察进而积累经验，掌握某些技能或者能力。\
![](figure/1.png)\
&ensp;&ensp;&ensp;&ensp;机器学习，实际上就是让计算机像人类一样，可以自动习得某些技能。说专业点，就是，让计算机通过观察大量的数据和训练，寻得某些模式，来自动习得某些skill。这里的skill主要是指的是系统的某些性能，比如说准确率之类的。总结一下，就是让计算机根据数据来总结经验，找出某种规律或模式，并用它来解决实际问题。\
![](figure/2.png)\
&ensp;&ensp;&ensp;&ensp;使用机器学习的场景：\
(1)事物本身存在某种潜在的模式\
(2)某些问题靠规则性的编程难以解决\
比如说声音、图像的识别\
(3)有大量样本可供使用\
2.Application of machine learning\
&ensp;&ensp;&ensp;&ensp;机器学习在衣食住行育乐方面有广泛的应用。比如说根据twiter上的数据来分析餐厅食物中毒的可能性；根据销售数据和用户调查来做服装搭配的推荐系统；根据建筑物特征及其负荷来预测其他建筑物的负荷；准确识别交通信号灯；根据学生在学习系统上的学习问答记录来预测学生答对某一到新题的可能性；根据用户喜欢的电影，来做电影推荐系统等。\
3.Components of Machine Learning\
&ensp;&ensp;&ensp;&ensp;基本的符号约束如下：\
（1）输入数据x\
（2）输出y，也就是输入数据的label\
（3）目标函数f，是实际样本的真实分布\
（4）假设h，一个model对应很多h，最佳的称为g，g能最好地表示事物的内在规律，使其接近于f。\
&ensp;&ensp;&ensp;&ensp;机器学习的流程总结如下：
可供训练的样本D，是由理想的目标函数f产生的，我们并不知道f具体是什么，我们需要根据先验知识选择模型，该模型有很多的假设集合H，H中包好了很多假设，通过算法在样本上不断学习，使其更接近于真实样本的产生，进而选出一个最好的假设，该假设对应的表达式g就是习得的模式。\
![](figure/3.png)\
4.Perceptron Hypothesis Set\
&ensp;&ensp;&ensp;&ensp;举一个例子：某银行要根据用户信息来决定是否要给申请的用户发放信用卡来获得尽可能多的收益。现在有训练样本D，主要是包括用户的信息和是否发了信用卡。这个问题可以通过机器学习解决，要根据D，选用某一算法A，在H中选择最好的h，得到g，接近目标函数f，然后利用这个习得的模型对新用户进行判断决定是否发放银行卡。本节课主要是基于感知机(Peceptron)进行总结。\

在是否发放信用卡的例子中，将用户个人信息作为输入数据x，代表的是每个用户的特征，总共有d个，在建立模型的时候，我们给予每个特征不同权重，表示的是该特征对输出影响程度。将所有的特征加权和的值和一个阈值threshold进行比较，如果大于这个阈值，输出为+1，即发信用卡；反之不发。
![](figure/4.png)\
&ensp;&ensp;&ensp;&ensp;为了表示方便，我们将阈值当做$w_0$\，于是h(x)可以表示为：
![](figure/5.png)\
以二维为例，该感知机模型在二维平面上就是一个分类的直线，直线一侧是+1，另一侧是-1，其中权重向量w则是这个直线的垂直线，即法向量。
![](figure/6.png)\
感知机线性分类在高纬空间中，则是超平面。\
5.Perceptron Learning Algorithm (PLA)\
&ensp;&ensp;&ensp;&ensp;H包含了许多h，即hypothesis set由许多条直线构成。通过算法A在数据上不断学习来选择一个最好的分界面，将样本尽可能地分正确。\
在这里，主要是通过逐点修正的方法来寻找最佳的分界面。以二维平面为例，在平面上随意取一条直线，看看哪些点分类错误。然后开始对第一个错误点就行修正，即变换直线的位置，使这个错误点变成分类正确的点。接着，再对第二个、第三个等所有的错误分类点就行直线纠正，直到所有的点都完全分类正确了，就得到了最好的直线。这种“逐步修正”，就是PLA思想所在。寻找到最佳的分界面。\
![](figure/7.png)\

如何修正正样本误分为负样本，即$w_t^Tx_{n(t)}<0$,$w$和$x$夹角大于90度，其中$w$是直线的法向量。一般的修正的方法就是使w和x夹角小于90度，通常做法就是做选取$w$和$x构成的的平行四边形作为更新后的$w$，如同上图所示。修正负样本误分为正样本原理与其相同。需要注意的是，每次修正直线，可能使之前分类正确的点变成错误点，这是可能发生的。但是没关系，不断迭代，不断修正，最终会将所有点完全正确分类（PLA前提是线性可分的）这种做法的思想是“知错能改”，即所谓的“A fault confessed is half redressed.”\

6.Guarantee of PLA\
&ensp;&ensp;&ensp;&ensp;关于PLA，我们需要考虑它什么时候可以停下来。在数据集线性可分的条件下，只要所有平面上的点都分类正确就停。针对线性可分的情况，假设有一条直线能满足停止条件，将目标权重记为$w_f$,对任意一点都满足：
![](figure/8.png)\
PLA会对每次错误的点进行修正，更新权重$w_{t+1}$的值，如果$w_{t+1}$与$w_f$越来越接近，内积越大，那表示$w_{t+1}是在接近目标权重$w_f$，证明PLA是有学习效果的。其内积可以表示为：
![](figure/9.png)\
内积更大，可能是向量长度更大了，不一定是向量间角度更小。所以，还需要证明$w_{t+1}$与$w_t$向量长度的关系
![](figure/10.png)\
从上述可知向量增长被限制了，跟新过后的和原来的不会差别太大。











